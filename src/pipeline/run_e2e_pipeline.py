"""
End2End evaluation script

Usage:
    run_e2e_pipeline.py [--note=<ExperimentSummary>] --predicting_head=<PredictionHeadName> --add_special_tokens=<AddBoundToks> --max_seq_len_query=<MaxSequence> --max_seq_len_passage=<MaxPassage> --batch_size_retriever=<BatchSizeRet> --batch_size_reader=<BatchSizeReader> --top_k_retriever=<TopKRetriever> --top_k_reader=<TopKReader> --query_model=<QueryModel> --passage_model=<PassageModel> --reader_model=<QueryModel> --gold_cluster_filename=<GoldClusterFile> --passages_filename=<PassagesFile> --query_filename=<QueryFile> --index_file=<IndexFile> --out_results_file=<OutResultsFile> --magnitude=<Magnitude>

Options:
    -h --help                                   Show this screen.
    --note=<ExperimentSummary>                  Brief note about the experiment
    --predicting_head=<PredictionHeadName>      Predicting head to use (kenton / dpr / bm25)
    --add_special_tokens=<AddBoundToks>         (true/false) whether to add span boundary tokens
    --max_seq_len_query=<MaxQuery>              Max query size
    --max_seq_len_passage=<MaxPassage>          Max passage size
    --batch_size_retriever=<BatchSizeRet>       Retriever batch size
    --batch_size_reader=<BatchSizeReader>       Reader batch size
    --top_k_retriever=<TopKRetriever>           The Top-K threshold for number of passages to return by the retriever
    --top_k_reader=<TopKReader>                 The Top-K threshold for number of passages to return by the reader
    --query_model=<QueryModel>                  Query model to evaluate
    --passage_model=<PassageModel>              Passage model to evaluate
    --reader_model=<QueryModel>                 Reader model to evaluate
    --gold_cluster_filename=<GoldClusterFile>   CoreSearch gold cluster file
    --query_filename=<QueryFile>                CoreSearch query file
    --passages_filename=<PassagesFile>          CoreSearch passage file
    --index_file=<IndexFile>                    Index file generated by evaluate_retriever.py
    --out_results_file=<OutResultsFile>         End2End evaluation report
    --magnitude=<Magnitude>                     Whether to run on just single cluster query or all the data (all/cluster)
"""

import json
import logging
import os
import pickle
from pathlib import Path
from typing import List, Dict

from docopt import docopt

from haystack.modeling.utils import set_all_seeds

from src.data_obj import Cluster, TrainExample, Passage, QueryResult
from src.index import elastic_index
from src.override_classes.reader.wec_reader import WECReader
from src.override_classes.retriever.wec_context_processor import WECContextProcessor
from src.override_classes.retriever.wec_dense import WECDensePassageRetriever
from src.pipeline.pipelines import QAPipeline, RetrievalOnlyPipeline
from src.utils import io_utils, measurments, data_utils, measure_squad
from src.utils.dpr_utils import create_file_doc_store
from src.utils.measurments import precision, precision_squad


logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)


SPLIT = "Test"


def main():
    predicting_head = _arguments.get("--predicting_head").lower()
    add_special_tokens = True if _arguments.get("--add_special_tokens").lower() == 'true' else False
    max_seq_len_query = int(_arguments.get("--max_seq_len_query"))
    max_seq_len_passage = int(_arguments.get("--max_seq_len_passage"))
    batch_size_retriever = int(_arguments.get("--batch_size_retriever"))
    batch_size_reader = int(_arguments.get("--batch_size_reader"))
    top_k_retriever = int(_arguments.get("--top_k_retriever"))
    top_k_reader = int(_arguments.get("--top_k_reader"))
    query_model = _arguments.get("--query_model")
    passage_model = _arguments.get("--passage_model")
    reader_model = _arguments.get("--reader_model")
    query_filename = _arguments.get("--query_filename")
    passages_filename = _arguments.get("--passages_filename")
    gold_cluster_filename = _arguments.get("--gold_cluster_filename")
    index_file = _arguments.get("--index_file")
    out_results_file = _arguments.get("--out_results_file")
    magnitude = _arguments.get("--magnitude").lower()

    if magnitude not in ["all", "cluster"]:
        raise ValueError(f"Value not support magnitude={magnitude}")

    num_processes = 1
    set_all_seeds(seed=42)

    base_result_file = os.path.splitext(out_results_file)[0]
    prediction_out_file = base_result_file + "_predictions.pickle"

    golds: List[Cluster] = io_utils.read_gold_file(gold_cluster_filename)
    query_examples: List[TrainExample] = io_utils.read_train_example_file(query_filename)

    if predicting_head in ["dpr", "kenton"]:
        document_store = create_file_doc_store(index_file, passages_filename)
        logger.info("Total indexed documents to be searched=" + str(document_store.get_document_count()))
        passage_dict = document_store.passages
        retriever = WECDensePassageRetriever(document_store=document_store, query_embedding_model=query_model,
                                             passage_embedding_model=passage_model,
                                             infer_tokenizer_classes=True,
                                             max_seq_len_query=max_seq_len_query,
                                             max_seq_len_passage=max_seq_len_passage,
                                             batch_size=batch_size_retriever, use_gpu=True, embed_title=False,
                                             use_fast_tokenizers=False, processor_type=WECContextProcessor,
                                             add_special_tokens=add_special_tokens)

        pipeline = QAPipeline(document_store=document_store,
                              retriever=retriever,
                              reader=WECReader(model_name_or_path=reader_model, use_gpu=True,
                                               num_processes=num_processes,
                                               add_special_tokens=add_special_tokens,
                                               batch_size=batch_size_reader,
                                               replace_prediction_heads=True,
                                               prediction_head_str=predicting_head),
                              ret_topk=top_k_retriever,
                              read_topk=top_k_reader)
    elif predicting_head == "bm25":
        document_store, retriever = elastic_index.load_wec_elastic_bm25(SPLIT.lower())
        pipeline = RetrievalOnlyPipeline(document_store=document_store,
                                         retriever=retriever,
                                         ret_topk=top_k_retriever)

        passage_examples: List[Passage] = io_utils.read_passages_file(passages_filename)
        passage_dict: Dict[str, Passage] = {obj.id: obj for obj in passage_examples}
    else:
        raise ValueError(f"Evaluation not supporting predicting head={predicting_head}")

    query_examples = generate_query_text(passage_dict, query_examples, predicting_head, magnitude)

    logger.info("Running QA pipeline..")
    predict_and_eval(pipeline, golds, query_examples, predicting_head, out_results_file, prediction_out_file)


def predict_and_eval(pipeline, golds, query_examples, run_pipe_str, result_out_file, predict_out_file):
    predictions: List[QueryResult] = pipeline.run_end_to_end(query_examples=query_examples)
    golds_arranged = data_utils.clusters_to_ids_list(gold_clusters=golds)

    # assert len(predictions) == len(golds_arranged)
    print_results(predictions, golds_arranged, run_pipe_str, result_out_file)
    with open(predict_out_file, 'wb') as output:
        pickle.dump(predictions, output, pickle.HIGHEST_PROTOCOL)


def generate_query_text(passage_dict: Dict[str, Passage], query_examples: List[TrainExample], query_method: str = None, magnitude="all"):
    used_clusters = set()
    ret_queries = list()
    logger.info("Using query style-" + query_method)
    for query in query_examples:
        if magnitude == "cluster" and query.goldChain in used_clusters:
            continue

        used_clusters.add(query.goldChain)
        ret_queries.append(query)
        if query_method:
            if query_method == "bm25":
                query.context = query.bm25_query.split(" ")
            else:
                pass

        if passage_dict:
            for pass_id in query.positive_examples:
                query.answers.add(" ".join(passage_dict[pass_id].mention))

    logger.info(f"Total generated queries-{len(ret_queries)}")
    return ret_queries


def print_results(predictions, golds_arranged, run_pipe_str, result_out_file):
    # Print retriever evaluation matrices
    to_print = print_measurements(predictions, golds_arranged, run_pipe_str)

    to_print.append("#################################################################################################")
    delimiter = "#################"
    for query_result in predictions:
        query_coref_id = str(query_result.query.goldChain)
        to_print.append(delimiter + " " + query_coref_id + " " + delimiter)
        query_context = " ".join(query_result.query.context)
        to_print.append("QUERY_ANSWERS=" + str(query_result.query.answers))
        to_print.append("QUERY_CONTEXT=" + query_context)
        to_print.append("QUERY_ID=" + query_result.query.id)
        for r_ind, result in enumerate(query_result.results[:10]):
            to_print.append("\tRESULT-" + str(result.id) + ":")
            to_print.append("\t\tCOREF_ID=" + str(result.goldChain))
            result_context = result.context
            result_answer = "NA_RETRIEVER"
            result_mention = result.mention
            if result.answer:
                result_answer = result.answer
            to_print.append("\t\tANSWER=" + str(result_answer))
            to_print.append("\t\tGOLD_MENTION=" + str(result_mention))
            to_print.append("\t\tSCORE=" + str(result.score))
            to_print.append("\t\tCONTEXT=" + str(result_context))

    join_result = "\n".join(to_print)
    # print(join_result)

    logger.info("Saving report to-" + result_out_file)
    with open(result_out_file, 'w') as f:
        f.write(join_result)


def print_measurements(predictions, golds_arranged, run_pipe_str=None):
    to_print = list()
    if run_pipe_str and run_pipe_str in ["dpr", "kenton"]:
        to_print.append("---------- Evaluation of Reader Model ------------")
        precision_method = precision_squad
        # Print the squad evaluation matrices
        to_print.append(json.dumps(measure_squad.eval_qa(predictions)))
        to_print.append("MRR@10=" + str(measurments.mean_reciprocal_rank(
            predictions=predictions, golds=golds_arranged, topk=10, method=run_pipe_str)))
        to_print.append("mAP@5=" + str(measurments.mean_average_precision(
            predictions=predictions, golds=golds_arranged, precision_method=precision_method, topk=5)))
        to_print.append("mAP@10=" + str(measurments.mean_average_precision(
            predictions=predictions, golds=golds_arranged, precision_method=precision_method, topk=10)))
        to_print.append("mAP@25=" + str(measurments.mean_average_precision(
            predictions=predictions, golds=golds_arranged, precision_method=precision_method, topk=25)))
        to_print.append("mAP@50=" + str(measurments.mean_average_precision(
            predictions=predictions, golds=golds_arranged, precision_method=precision_method, topk=50)))
    to_print.append("---------- Evaluation of Retriever Model ------------")
    precision_method = precision
    to_print.append("MRR@10=" + str(measurments.mean_reciprocal_rank(
        predictions=predictions, golds=golds_arranged, topk=10, method=run_pipe_str)))
    to_print.append("mAP@5=" + str(measurments.mean_average_precision(
        predictions=predictions, golds=golds_arranged, precision_method=precision_method, topk=5)))
    to_print.append("mAP@10=" + str(measurments.mean_average_precision(
        predictions=predictions, golds=golds_arranged, precision_method=precision_method, topk=10)))
    to_print.append("mAP@25=" + str(measurments.mean_average_precision(
        predictions=predictions, golds=golds_arranged, precision_method=precision_method, topk=25)))
    to_print.append("mAP@50=" + str(measurments.mean_average_precision(
        predictions=predictions, golds=golds_arranged, precision_method=precision_method, topk=50)))
    to_print.append("mAP@100=" + str(measurments.mean_average_precision(
        predictions=predictions, golds=golds_arranged, precision_method=precision_method, topk=100)))

    to_print.append("Recall@5=" + str(measurments.recall(
        predictions=predictions, golds=golds_arranged, topk=5)))
    to_print.append("Recall@10=" + str(measurments.recall(
        predictions=predictions, golds=golds_arranged, topk=10)))
    to_print.append("Recall@25=" + str(measurments.recall(
        predictions=predictions, golds=golds_arranged, topk=25)))
    to_print.append("Recall@50=" + str(measurments.recall(
        predictions=predictions, golds=golds_arranged, topk=50)))
    to_print.append("Recall@100=" + str(measurments.recall(
        predictions=predictions, golds=golds_arranged, topk=100)))
    to_print.append("Recall@200=" + str(measurments.recall(
        predictions=predictions, golds=golds_arranged, topk=200)))
    to_print.append("Recall@300=" + str(measurments.recall(
        predictions=predictions, golds=golds_arranged, topk=300)))
    to_print.append("Recall@400=" + str(measurments.recall(
        predictions=predictions, golds=golds_arranged, topk=400)))
    to_print.append("Recall@500=" + str(measurments.recall(
        predictions=predictions, golds=golds_arranged, topk=500)))
    return to_print


if __name__ == '__main__':
    _arguments = docopt(__doc__, argv=None, help=True, version=None, options_first=False)
    print(_arguments)
    main()
