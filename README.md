# CoreSearch
This project is following our research paper: Cross-document Event Coreference Search: Task, Dataset and Modeling <LINK-TBD> 

## Features
1. CoreSearch Dataset
2. CoreSearch Task
3. Models: 
   1. Retriever
   2. Reader

## Installation
Installation from the source. Python's virtual or Conda environments are recommended.
```bash
git clone https://github.com/AlonEirew/CoreSearch.git
cd CoreSearch
pip install -e .
```

## Retriever training
Training retriever moder require the CoreSearch data in DPR format.
Full argument description is available in the top of `train_retriever.py` script. 
```bash
python src/train/train_retriever.py \
    --doc_dir data/resources/dpr/context/ \
    --train_filename Train.json \
    --dev_filename Dev.json \
    --checkpoint_dir data/checkpoints/ \
    --output_model Retriever_SpanBERT \
    --add_special_tokens true \
    --n_epochs 5 \
    --max_seq_len_query 64 \
    --max_seq_len_passage 180 \
    --batch_size 64 \
    --query_model SpanBERT/spanbert-base-cased \
    --passage_model SpanBERT/spanbert-base-cased \
    --evaluate_every 500
```

## Reader training
Training reader moder require the CoreSearch data in SQuAD format.
Full argument description is available in the top of `train_reader.py` script.
```bash
python src/train/train_reader.py \
    --doc_dir data/resources/squad/context \ 
    --train_filename Train_squad_format_1pos_23neg.json \ 
    --dev_filename Dev_squad_format_1pos_23neg.json \ 
    --checkpoint_dir data/checkpoints/ \ 
    --output_model Reader-RoBERTa_base_Kenton \ 
    --predicting_head kenton \
    --num_processes 10 \ 
    --add_special_tokens true \ 
    --n_epochs 5 \
    --max_seq_len 256 \
    --max_seq_len_query 64 \
    --batch_size 64 \
    --reader_model roberta-base \
    --evaluate_every 750
```

## Run Retriever Evaluation
This script is for evaluating the retriever model and generating a file index for the top-k results of every question.
Information on parameters can be found in the top of `evaluate_retriever.py` script.

```bash
python src/evaluation/evaluate_retriever.py \
    --query_filename data/resources/WEC-ES/train/Dev_queries.json \
    --passages_filename data/resources/WEC-ES/clean/Dev_all_passages.json \
    --gold_cluster_filename=data/resources/WEC-ES/clean/Dev_gold_clusters.json \
    --query_model data/checkpoints/Retriever_SpanBERT_notoks_5it/0/query_encoder \
    --passage_model data/checkpoints/Retriever_SpanBERT_notoks_5it/0/passage_encoder \
    --out_index_file file_indexes/Dev_Retriever_spanbert_notoks_5it0_top500.json \
    --out_results_file file_indexes/Dev_Retriever_spanbert_notoks_5it0_top500_results.txt \
    --num_processes -1 \
    --add_special_tokens true \
    --max_seq_len_query 64 \
    --max_seq_len_passage 180 \
    --batch_size 240 \
    --top_k 500
```

## Run End2End Pipeline
**_Prerequisites:_** Generating an index to retrieve from, index can be generated by running `evaluate_retriever.py`, or by creating an elastic index using `elastic_index.py` (detailed below) for BM25 retriever.

Running the end2end pipeline.
Information on parameters can be found in the top of `run_e2e_pipeline.py` script.
```bash
python src/pipeline/run_e2e_pipeline.py \
  --predicting_head kenton \
  --max_seq_len_query 64 \
  --max_seq_len_passage 180 \
  --add_special_tokens true \
  --batch_size_retriever 24 \
  --batch_size_reader 24 \
  --top_k_retriever 500 \
  --top_k_reader 50 \
  --query_model data/checkpoints/Retriever_SpanBERT_5it/1/query_encoder \
  --passage_model data/checkpoints/Retriever_SpanBERT_5it/1/passage_encoder \
  --reader_model data/checkpoints/Reader-RoBERTa_base_Kenton_special/1 \
  --query_filename data/resources/WEC-ES/train/Dev_queries.json \
  --passages_filename data/resources/WEC-ES/clean/Dev_all_passages.json --gold_cluster_filename data/resources/WEC-ES/clean/Dev_gold_clusters.json --index_file file_indexes/Dev_Retriever_spanbert_5it1_top500.json --out_results_file results/Dev_Retriever_spanbert_5it1.txt --magnitude all
```

## Elastic Index for BM25 Retriever

### elastic_index.py
This script will create a new ElasticSearch index containing documents generated from the input file.
In case given index already exists, it will be deleted by this process and recreated.

**_Prerequisite:_** Pulling elastic image and running it:
```bash
docker pull docker.elastic.co/elasticsearch/elasticsearch:7.9.2
docker run -d -p 9200:9200 -e "discovery.type=single-node" elasticsearch:7.9.2
```
    
**After Index is up and running:**
```bash
python src/index/elastic_index.py \
  --input=data/resources/WEC-ES/clean/Train_all_passages.json \
  --index=train
```

## Additional Scripts
#### DPR Files Generation
Generate DPR Files from CoreSearch files:<br/>
`python scripts/to_dpr_format.py`

#### SQuAD Files Generation
Generate SQuAD Files from CoreSearch files:<br/>
`python scripts/to_squad_format.py`

## Run Full Experiments Pipeline:
1) Run **retriever** training -- `src/train/train_retriever.py`
2) Run Evaluation and Index script on DEV to generate results and passage index: -- `src/evaluation/evaluate_retriever.py`
3) Take the best model and generate TRAIN and TEST index (using above `evaluate_retriever.py` script)
4) Generate Squad files using script -- `scripts/to_squad_format.py`
5) Run **reader** training -- `src/train/train_reader.py`
6) Run full pipeline on DEV set of retriever/reader -- `src/pipeline/run_haystack_pipeline.py`
7) Run full pipeline with best model on TEST set

